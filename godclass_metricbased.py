# -*- coding: utf-8 -*-
"""ReadMLCQ_GodClass_MetricBased.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CSqZXCJZJPyK_Hs-GU-GILZYd2tmlreN
"""

import pandas as pd

# Read the dataset into a DataFrame
df = pd.read_csv('MLCQCodeSmellSamples.csv', delimiter=';')

# Count the unique sample_id values
unique_sample_id_count = df['sample_id'].nunique()

# Print the count
print("Number of unique sample_id:", unique_sample_id_count)

import pandas as pd

# Read the dataset into a DataFrame
df = pd.read_csv('MLCQCodeSmellSamples.csv', delimiter=';')

# Filter the DataFrame by type = "function"
filtered_df = df[df['type'] == 'function']

# Count the unique sample_id values
unique_sample_id_count = filtered_df['sample_id'].nunique()

# Print the count
print("Number of unique sample_id (type=function):", unique_sample_id_count)

import pandas as pd

# Read the dataset into a DataFrame
df = pd.read_csv('MLCQCodeSmellSamples.csv', delimiter=';')

# Filter the DataFrame by type = "class"
filtered_df = df[df['type'] == 'class']

# Count the unique sample_id values
unique_sample_id_count = filtered_df['sample_id'].nunique()

# Print the count
print("Number of unique sample_id (type=class):", unique_sample_id_count)

# Filter the DataFrame by smell = "blob"
god_class_df = df[df['smell'] == 'blob']

# Print the god_class_df
display(god_class_df)

#Blob
import pandas as pd

# Read the dataset into a DataFrame
df = pd.read_csv('MLCQCodeSmellSamples.csv', delimiter=';')

# Filter the DataFrame by smell = "blob"
god_class_df = df[df['smell'] == 'blob']

# Get unique sample_id values
unique_sample_ids = god_class_df['sample_id'].unique()

# Initialize a list to store the unique_god_class_df rows
unique_rows = []

# Iterate over unique sample_ids
for sample_id in unique_sample_ids:
    # Filter by sample_id
    sample_rows = god_class_df[god_class_df['sample_id'] == sample_id]
    #display(sample_rows)
    # Retrieve start_line, end_line, and link values
    start_line = sample_rows['start_line'].iloc[0]
    end_line = sample_rows['end_line'].iloc[0]
    link = sample_rows['link'].iloc[0]


    #0 when all values for the same sample_id in severity column = none 1 otherwise
    # Determine label based on severity values
    severities = sample_rows['severity'].unique()
    #display(severities)
    label = 0 if len(severities) == 1 and severities[0] == 'none' else 1

    # Create a row for unique_god_class_df
    unique_row = {'sample_id': sample_id, 'start_line': start_line, 'end_line': end_line, 'link': link, 'label': label}

    # Append the row to unique_rows list
    unique_rows.append(unique_row)

# Create the unique_god_class_df DataFrame
unique_god_class_df = pd.DataFrame(unique_rows)

# Print the unique_god_class_df
print(unique_god_class_df)

display(unique_god_class_df)

# Count the number of 0s and 1s in the label column of unique_god_class_df
label_counts = unique_god_class_df['label'].value_counts()

# Print the counts
print(label_counts)



import requests
# Clean the link column by removing the unwanted information
unique_god_class_df['link'] = unique_god_class_df['link'].str.rsplit('/#L', n=1).str[0]

# Add a new column 'code_snippet' to the DataFrame
unique_god_class_df['code_snippet'] = ''

# Iterate over the rows in the DataFrame
for index, row in unique_god_class_df.iterrows():
    link = row['link']
    start_line = row['start_line']
    end_line = row['end_line']

    # Send a GET request to retrieve the raw file content from GitHub
    raw_url = link.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
    response = requests.get(raw_url)

    # Check if the request was successful
    if response.status_code == 200:
        # Split the response content into lines
        lines = response.text.split("\n")

        # Extract the lines specified by start_line and end_line
        snippet_lines = lines[start_line - 1:end_line]

        # Join the code snippet lines into a single string
        snippet = "\n".join(snippet_lines)

        # Assign the snippet to the 'code_snippet' column
        unique_god_class_df.at[index, 'code_snippet'] = snippet
    else:
        print(f"Failed to retrieve code from link: {link}")

# Print the updated DataFrame
print(unique_god_class_df)

# Count the number of rows with snippets
rows_with_snippets = unique_god_class_df['code_snippet'].notnull().sum()

# Count the number of rows without snippets
rows_without_snippets = unique_god_class_df['code_snippet'].isnull().sum()

# Print the results
print(f"Rows with snippets: {rows_with_snippets}")
print(f"Rows without snippets: {rows_without_snippets}")

# Filter the DataFrame to get the row with sample_id = 8719536
snippet_row = unique_god_class_df[unique_god_class_df['sample_id'] == 8719536]

# Retrieve the code snippet
snippet = snippet_row['code_snippet'].values[0]

# Print the code snippet
print(snippet)

# Count the number of rows with snippets
rows_with_snippets = len(unique_god_class_df[unique_god_class_df['code_snippet'] != ''])
print("Rows with snippets:", rows_with_snippets)

# Count the number of rows without snippets
rows_without_snippets = len(unique_god_class_df[unique_god_class_df['code_snippet'] == ''])
print("Rows without snippets:", rows_without_snippets)

# Drop rows without snippets
unique_god_class_df = unique_god_class_df[unique_god_class_df['code_snippet'] != '']

# Reset the index
unique_god_class_df.reset_index(drop=True, inplace=True)

# Print the updated DataFrame
print(unique_god_class_df)

# Count the number of rows with snippets
rows_with_snippets = len(unique_god_class_df[unique_god_class_df['code_snippet'] != ''])
print("Rows with snippets:", rows_with_snippets)

# Count the number of rows without snippets
rows_without_snippets = len(unique_god_class_df[unique_god_class_df['code_snippet'] == ''])
print("Rows without snippets:", rows_without_snippets)

# Specify the output CSV file path
output_csv_path = 'unique_god_class_df.csv'

# Save the DataFrame to CSV
unique_god_class_df.to_csv(output_csv_path, index=False)

#Start Load CSV GodClass and train from here....
#Apply metricBased

#start parsed blob and calc metrics from here

!pip install javalang

import pandas as pd

# Specify the path to your CSV file
csv_file_path = 'unique_god_class_df.csv'

# Read the CSV file into a DataFrame
unique_god_class_df = pd.read_csv(csv_file_path)

# Print the DataFrame
display(unique_god_class_df)

import javalang

def count_statements(sample_id, code):
    try:
        # Tokenize and parse the evaluated code
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        # Count the statements
        count = 0
        for _, node in tree:
            if isinstance(node, javalang.tree.Statement):
                count += 1

        return count

    except javalang.parser.JavaSyntaxError:
        print("Syntax error in sample_id:", sample_id)
        return None

# Apply the count_statements function to the code snippets in unique_god_class_df
unique_god_class_df['count_statement'] = unique_god_class_df.apply(lambda row: count_statements(row['sample_id'], row['code_snippet']), axis=1)

# Print the updated unique_god_class_df
print(unique_god_class_df)

num_nan = unique_god_class_df['count_statement'].isna().sum()
print(num_nan)

parsed_unique_god_class_df = unique_god_class_df.dropna()

parsed_unique_god_class_df.describe()

parsed_unique_god_class_df

import pandas as pd

# Specify the path to your CSV file
csv_file_path = 'parsed_unique_god_class_df.csv'

# Read the CSV file into a DataFrame
unique_god_class_df = pd.read_csv(csv_file_path)

# Print the DataFrame
display(unique_god_class_df)

# Specify the output CSV file path
output_csv_path = 'parsed_unique_god_class_df.csv'

# Save the DataFrame to CSV
parsed_unique_god_class_df.to_csv(output_csv_path, index=False)

#count loop statments
import ast



def count_loop_statements(code):
    # Evaluate the code using ast.literal_eval()
    #code_evaluated = ast.literal_eval('"""' + code + '"""')

    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()

    loop_count = 0

    for _, node in tree:
        if isinstance(node, javalang.tree.ForStatement) or \
           isinstance(node, javalang.tree.WhileStatement) or \
           isinstance(node, javalang.tree.DoStatement) or \
           isinstance(node, javalang.tree.EnhancedForControl):
            loop_count += 1

    return loop_count

# Create a new column 'count_loop_statements' in the DataFrame 'df'
parsed_unique_god_class_df['count_loop_statements'] = parsed_unique_god_class_df['code_snippet'].apply(count_loop_statements)
display(parsed_unique_god_class_df)

#count_nodes
import ast



def count_nodes(code):

    # Evaluate the code using ast.literal_eval()
    #code_evaluated = ast.literal_eval('"""' + code + '"""')

    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()

    node_count = 0

    for _, node in tree:
        node_count += 1

    return node_count


# Create a new column 'count_nodes' in the DataFrame 'df'
parsed_unique_god_class_df['count_nodes'] = parsed_unique_god_class_df['code_snippet'].apply(count_nodes)
display(parsed_unique_god_class_df)

#count number of conditional statement


def count_conditional_statements(code):
    # Evaluate the code using ast.literal_eval()
    #code_evaluated = ast.literal_eval('"""' + code + '"""')

    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()


    conditional_count = 0

    for _, node in tree:
        if isinstance(node, javalang.tree.IfStatement) or isinstance(node, javalang.tree.SwitchStatement):
            conditional_count += 1

    return conditional_count

# Create a new column 'count_conditional_statements' in the DataFrame 'df'
parsed_unique_god_class_df['count_conditional_statements'] = parsed_unique_god_class_df['code_snippet'].apply(count_conditional_statements)
display(parsed_unique_god_class_df)

#calculate_method_fan_out
import ast


def calculate_method_fan_out(code):
    try:
        # Evaluate the code using ast.literal_eval()
        #code_evaluated = ast.literal_eval('"""' + code + '"""')

        # Tokenize and parse the evaluated code
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        # Count the method fan-outs
        fan_out_count = 0
        for _, node in tree:
            if isinstance(node, javalang.tree.MethodDeclaration):
                fan_out_count += len(node.parameters)

        return fan_out_count

    except (SyntaxError, ValueError) as e:
        print("Error evaluating code:")
        print(code)
        raise e

# Create a new column 'method_fan_out' in the DataFrame 'df'
parsed_unique_god_class_df['method_fan_out'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_method_fan_out)
display(parsed_unique_god_class_df)

#check have exception handling or not


def has_exception_handling(code):
    try:
        # Evaluate the code using ast.literal_eval()
        #code_evaluated = ast.literal_eval('"""' + code + '"""')

        # Tokenize and parse the evaluated code
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        for path, node in tree:
            if isinstance(node, javalang.tree.TryStatement) or isinstance(node, javalang.tree.CatchClause) or isinstance(node, javalang.tree.ThrowStatement):
                return 1

        return 0

    except (SyntaxError, ValueError):
        return -1


parsed_unique_god_class_df['has_exception_handling']  = parsed_unique_god_class_df['code_snippet'].apply(has_exception_handling)
display(parsed_unique_god_class_df)

#count_return_statements


def count_return_statements(code):
    try:
        # Evaluate the code using ast.literal_eval()
        #code_evaluated = ast.literal_eval('"""' + code + '"""')

        # Tokenize and parse the evaluated code
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        # Count the return statements
        count = 0
        for _, node in tree:
            if isinstance(node, javalang.tree.ReturnStatement):
                count += 1
        return count

    except (SyntaxError, ValueError) as e:
        print("Error evaluating code:")
        print(code)
        raise e

# Create a new column 'count_return_statements' in the DataFrame 'df'
parsed_unique_god_class_df['count_return_statements'] = parsed_unique_god_class_df['code_snippet'].apply(count_return_statements)
display(parsed_unique_god_class_df)

#count Number of local variables


def count_local_variables(code):
    # Evaluate the code using ast.literal_eval()
    #code_evaluated = ast.literal_eval('"""' + code + '"""')

    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()
    local_variable_count = 0

    for path, node in tree:
        if isinstance(node, javalang.tree.LocalVariableDeclaration):
            local_variable_count += len(node.declarators)

    return local_variable_count

# Create a new column 'count_local_variables' in the DataFrame 'df'
parsed_unique_god_class_df['count_local_variables'] = parsed_unique_god_class_df['code_snippet'].apply(count_local_variables)
display(parsed_unique_god_class_df)

#count_methods
def count_methods(code):
    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()
    method_count = 0
    for path, node in tree:
        if isinstance(node, javalang.tree.MethodDeclaration):
            method_count += 1
    return method_count

# Create a new column 'count_methods' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['count_methods'] = parsed_unique_god_class_df['code_snippet'].apply(count_methods)
display(parsed_unique_god_class_df)

#count_fields
def count_fields(code):
    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()
    field_count = 0
    for path, node in tree:
        if isinstance(node, javalang.tree.FieldDeclaration):
            field_count += len(node.declarators)
    return field_count

# Create a new column 'count_fields' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['count_fields'] = parsed_unique_god_class_df['code_snippet'].apply(count_fields)
display(parsed_unique_god_class_df)

def calculate_tcc(code):
    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()

    method_pairs = 0
    common_method_pairs = 0

    for path, node in tree:
        if isinstance(node, javalang.tree.MethodDeclaration):
            method_pairs += 1
            method_invocations = set()
            for _, invocation_node in node.filter(javalang.tree.MethodInvocation):
                method_invocations.add(invocation_node.member)

            for _, other_node in tree:
                if isinstance(other_node, javalang.tree.MethodDeclaration):
                    if other_node.name != node.name:
                        other_method_invocations = set()
                        for _, invocation_node in other_node.filter(javalang.tree.MethodInvocation):
                            other_method_invocations.add(invocation_node.member)
                        if method_invocations.intersection(other_method_invocations):
                            common_method_pairs += 1

    if method_pairs > 0:
        tcc = common_method_pairs / method_pairs
    else:
        tcc = 0

    return tcc

# Create a new column 'tcc' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['tcc'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_tcc)
display(parsed_unique_god_class_df)

def count_lines_of_code(code):
    # Split the code into lines
    lines = code.split('\n')

    # Count the non-empty lines of code
    loc = sum(1 for line in lines if line.strip() != '')

    return loc

# Create a new column 'loc' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['loc'] = parsed_unique_god_class_df['code_snippet'].apply(count_lines_of_code)
display(parsed_unique_god_class_df)

def calculate_atfd(code):
    # Tokenize and parse the evaluated code
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()

    atfd = 0  # Initialize the ATFD metric to 0

    field_declarations = set()  # Store the field declarations in the class

    for path, node in tree:
        if isinstance(node, javalang.tree.FieldDeclaration):
            # Add the field names to the set of field declarations
            for declarator in node.declarators:
                field_declarations.add(declarator.name)

        if isinstance(node, javalang.tree.MethodDeclaration):
            for parameter in node.parameters:
                # Check if the parameter type is from an unrelated class
                if parameter.type.name not in field_declarations:
                    atfd += 1  # Increment the ATFD metric

            if node.body is not None:
                for statement in node.body:
                    # Check if the statement is a method invocation
                    if isinstance(statement, javalang.tree.MethodInvocation):
                        # Check if the invoked method belongs to an unrelated class
                        if statement.member not in field_declarations:
                            atfd += 1  # Increment the ATFD metric

    return atfd

# Create a new column 'atfd' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['atfd'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_atfd)
display(parsed_unique_god_class_df)

def calculate_method_fan_out(code):
    try:
        # Tokenize and parse the evaluated code
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        # Set to store the unique classes accessed by the method
        accessed_classes = set()

        # Traverse the AST to identify accessed classes
        for _, node in tree:
            if isinstance(node, javalang.tree.MethodDeclaration):
                if node.body:
                    for statement in node.body:
                        if isinstance(statement, javalang.tree.VariableDeclaration):
                            accessed_classes.add(statement.type.name)
                        elif isinstance(statement, javalang.tree.MethodInvocation):
                            accessed_classes.add(statement.qualifier)

        return len(accessed_classes)

    except (SyntaxError, ValueError) as e:
        print("Error evaluating code:")
        print(code)
        raise e

# Create a new column 'method_fan_out' in the DataFrame 'df'
parsed_unique_god_class_df['method_fan_out'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_method_fan_out)
display(parsed_unique_god_class_df)

def calculate_cyclomatic_complexity(code):
    try:
        # Tokenize and parse the evaluated code
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        # Count the number of decision points (branches and loops)
        complexity = 1  # Start with a complexity of 1 for the method itself
        for _, node in tree:
            if isinstance(node, javalang.tree.IfStatement):
                complexity += 1
            elif isinstance(node, javalang.tree.ForStatement):
                complexity += 1
            elif isinstance(node, javalang.tree.WhileStatement):
                complexity += 1
            elif isinstance(node, javalang.tree.DoStatement):
                complexity += 1
            elif isinstance(node, javalang.tree.SwitchStatement):
                complexity += len(node.cases)

        return complexity

    except (SyntaxError, ValueError) as e:
        print("Error evaluating code:")
        print(code)
        raise e

# Create a new column 'cyclomatic_complexity' in the DataFrame 'df'
parsed_unique_god_class_df['cyclomatic_complexity'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_cyclomatic_complexity)
display(parsed_unique_god_class_df)

def calculate_lcom5(code):
    # Tokenize and parse the code snippet
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()

    # Collect all the method names and instance variables
    methods = {}
    variables = set()
    for _, node in tree:
        if isinstance(node, javalang.tree.MethodDeclaration):
            methods[node.name] = set()
            for path, child_node in node:
                if isinstance(child_node, javalang.tree.VariableDeclarator):
                    variables.add(child_node.name)
                    methods[node.name].add(child_node.name)

    # Calculate the LCOM5 metric
    lcom5 = 0
    for method1, vars1 in methods.items():
        for method2, vars2 in methods.items():
            if method1 != method2:
                common_vars = vars1.intersection(vars2)
                if len(common_vars) == 0:
                    lcom5 += 1

    return lcom5

# Create a new column 'lcom5' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['lcom5'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_lcom5)

display(parsed_unique_god_class_df)

def calculate_rfc(code):
    # Tokenize and parse the code snippet
    tokens = javalang.tokenizer.tokenize(code)
    parser = javalang.parser.Parser(tokens)
    tree = parser.parse_member_declaration()

    # Collect all the method names
    methods = set()#using set to store unique methods only to achieve difinition of rfc
    for _, node in tree:
        if isinstance(node, javalang.tree.MethodDeclaration):
            methods.add(node.name)

    # Calculate the RFC metric
    rfc = len(methods)
    return rfc

# Create a new column 'rfc' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['rfc'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_rfc)

display(parsed_unique_god_class_df)

def calculate_lcom(code):
    try:
        # Tokenize and parse the code snippet
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        # Initialize variables
        method_attributes = {}  # Dictionary to store attributes accessed by each method

        # Iterate over methods in the class
        for _, node in tree:
            if isinstance(node, javalang.tree.MethodDeclaration):
                method_name = node.name
                method_attributes[method_name] = set()

                # Skip methods without a body
                if node.body is None:
                    continue

                # Traverse the method body to find attribute accesses
                for statement in node.body:
                    if isinstance(statement, javalang.tree.StatementExpression):
                        expression = statement.expression
                        if isinstance(expression, javalang.tree.Assignment) and isinstance(expression.value, javalang.tree.MemberReference):
                            # Add the accessed attribute to the method's set
                            method_attributes[method_name].add(expression.value.member)

        # Calculate LCOM value
        num_methods = len(method_attributes)
        if num_methods > 1:
            sum_differences = sum(len(method_attributes[method]) for method in method_attributes) - num_methods * len(set.union(*method_attributes.values()))
            lcom = sum_differences / (num_methods * (num_methods - 1))
        else:
            lcom = 0

        return lcom

    except (SyntaxError, ValueError) as e:
        print("Error evaluating code:")
        print(code)
        raise e

# Create a new column 'lcom' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['lcom'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_lcom)
display(parsed_unique_god_class_df)

def calculate_cbo(code):
    try:
        # Tokenize and parse the code snippet
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse_member_declaration()

        # Find the class declaration
        for path, node in tree:
            if isinstance(node, javalang.tree.ClassDeclaration):
                # Get the direct references to other classes
                direct_references = set()
                for member in node.body:
                    if isinstance(member, javalang.tree.TypeDeclaration):
                        direct_references.add(member.name)

                # Calculate CBO
                cbo = len(direct_references)
                return cbo

        # Return 0 if there are no direct references
        return 0

    except (SyntaxError, ValueError) as e:
        print("Error evaluating code:")
        print(code)
        raise e

# Create a new column 'cbo' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['cbo'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_cbo)
display(parsed_unique_god_class_df)

def calculate_dit(code):
    try:
        # Tokenize and parse the code snippet
        tokens = javalang.tokenizer.tokenize(code)
        parser = javalang.parser.Parser(tokens)
        tree = parser.parse()

        # Find the class declaration
        class_declarations = [node for path, node in tree if isinstance(node, javalang.tree.ClassDeclaration)]

        if len(class_declarations) > 0:
            # Get the superclass for the first class declaration
            superclass_name = class_declarations[0].extends.name if class_declarations[0].extends else None

            # Traverse the inheritance hierarchy to find the DIT
            dit = 1  # Start with a DIT of 1 for the class itself
            while superclass_name:
                dit += 1
                superclass = next((c for c in class_declarations if c.name == superclass_name), None)
                if superclass:
                    superclass_name = superclass.extends.name if superclass.extends else None
                else:
                    superclass_name = None

            return dit
        else:
            return 0

    except (SyntaxError, ValueError) as e:
        print("Error evaluating code:")
        print(code)
        raise e

# Create a new column 'dit' in the DataFrame 'parsed_unique_god_class_df'
parsed_unique_god_class_df['dit'] = parsed_unique_god_class_df['code_snippet'].apply(calculate_dit)
display(parsed_unique_god_class_df)

print(parsed_unique_god_class_df.dtypes)

parsed_unique_god_class_df

count_values = parsed_unique_god_class_df['label'].value_counts()
print(count_values)

nan_counts = parsed_unique_god_class_df.isna().sum()
print(nan_counts)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Select the relevant columns for analysis
columns_of_interest = ['code_snippet', 'count_statement', 'count_loop_statements', 'count_nodes',
                       'count_conditional_statements', 'method_fan_out', 'has_exception_handling',
                       'count_return_statements', 'count_local_variables', 'count_methods',
                       'count_fields', 'tcc', 'loc', 'atfd', 'cyclomatic_complexity',
                       'lcom5', 'rfc', 'lcom', 'cbo', 'dit', 'label']

# Create a new DataFrame with the selected columns
data = parsed_unique_god_class_df[columns_of_interest].copy()

# Convert the code_snippet column to numeric vector representation (if needed)
# ...

# Split the data into input features (X) and the target variable (y)
X = data.drop(['code_snippet', 'label'], axis=1)
y = data['label']

# Train a Random Forest classifier
model = RandomForestClassifier()
model.fit(X, y)

# Get the feature importances
feature_importances = model.feature_importances_

# Create a DataFrame to show the importance of each column
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values('Importance', ascending=False)

# Print the importance of each column
print(importance_df)

import pandas as pd
"""
most researchers characterize
God Classes using size (NOM, NOF, CLOC) and complexity
metrics, while some also include coupling (ATFD) and cohesion (LCOM)
measures.
"""
# Select the desired columns as features
features = ['count_methods','count_fields','tcc','cyclomatic_complexity', 'loc','atfd', 'lcom5']

# Select the 'label' column as the target
target = 'label'

# Define the features and target variables
features = parsed_unique_god_class_df[features]
target = parsed_unique_god_class_df[target]

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import f1_score
import numpy as np
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define the models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True),
    'Decision Tree': DecisionTreeClassifier(),
    'Naive Bayes': GaussianNB(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'xgb': XGBClassifier()
}

# Initialize empty lists to store the evaluation metrics
model_names = []
accuracies = []
precisions = []
recalls = []
fscores = []

# Perform 10-fold cross-validation for each model
for model_name, model in models.items():

    # Initialize empty lists to store the evaluation metrics for each fold
    fold_accuracies = []
    fold_precisions = []
    fold_recalls = []
    fold_fscores = []

    # Perform 10-fold cross-validation
    kfold = KFold(n_splits=10, shuffle=True, random_state=42)

    for train_index, test_index in kfold.split(features):
        X_train, X_test = features.iloc[train_index], features.iloc[test_index]
        y_train, y_test = target.iloc[train_index], target.iloc[test_index]

        # Scale the features
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Apply SMOTE for oversampling
        smote = SMOTE()
        X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)
        X_train = X_train_oversampled
        y_train = y_train_oversampled

        # Fit the model on the training data
        model.fit(X_train, y_train)

        # Predict the target variable for the test data
        y_pred = model.predict(X_test)

        # Calculate evaluation metrics for the fold
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        fscore = f1_score(y_test, y_pred)

        # Append the metrics to the fold lists
        fold_accuracies.append(accuracy)
        fold_precisions.append(precision)
        fold_recalls.append(recall)
        fold_fscores.append(fscore)

    # Calculate the mean metrics across all folds
    mean_accuracy = np.mean(fold_accuracies)
    mean_precision = np.mean(fold_precisions)
    mean_recall = np.mean(fold_recalls)
    mean_fscore = np.mean(fold_fscores)

    # Append the mean metrics to the overall lists
    model_names.append(model_name)
    accuracies.append(mean_accuracy)
    precisions.append(mean_precision)
    recalls.append(mean_recall)
    fscores.append(mean_fscore)

# Create a comparison table
results_df = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracies,
    'Precision': precisions,
    'Recall': recalls,
    'F1-score': fscores
})

# Print the comparison table
print("Comparison Table:")
display(results_df)
print()
print()

# Display the comparison table as a bar plot
results_df.plot(kind='bar', x='Model', figsize=(10, 6))
plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Comparison of Different Models')
plt.legend(loc='lower right')
plt.xticks(rotation=45)
plt.show()
print()

# Print the evaluation results
print("Evaluation Results:")
for i in range(len(model_names)):
    model_name = model_names[i]
    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracies[i]:.4f}")
    print(f"Precision: {precisions[i]:.4f}")
    print(f"Recall: {recalls[i]:.4f}")
    print(f"F1-score: {fscores[i]:.4f}")
    print()

"""
from sklearn.model_selection import train_test_split
# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(features,target, test_size=0.2, random_state=42)

# Print the shapes of the training and test sets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""
from sklearn.preprocessing import StandardScaler
#Scale the combined features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#joblib.dump(scaler, 'scaler.pkl')

"""
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)
X_train = X_train_oversampled
y_train = y_train_oversampled

# html web page...
# Train machine learning models no grid
# Define the models
# features and target here (in metric based) are DataFrames not NumPy arrays so i used iloc.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from imblearn.combine import SMOTEENN
from sklearn.metrics import classification_report, accuracy_score

models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True),
    'Decision Tree': DecisionTreeClassifier(),
    'Naive Bayes': GaussianNB(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'xgb': XGBClassifier()
}

# Initialize empty lists to store the evaluation metrics
model_names = []
accuracies = []
precisions = []
recalls = []
fscores = []

# Perform 10-fold cross-validation for each model
for model_name, model in models.items():

    # Initialize empty lists to store the evaluation metrics for each fold
    fold_accuracies = []
    fold_precisions = []
    fold_recalls = []
    fold_fscores = []

    # Perform 10-fold cross-validation
    kfold = KFold(n_splits=10, shuffle=True, random_state=42)

    for train_index, test_index in kfold.split(features):
        X_train, X_test = features.iloc[train_index], features.iloc[test_index]
        y_train, y_test = target.iloc[train_index], target.iloc[test_index]

        # Scale the features
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Apply SMOTE for oversampling
        smote = SMOTE()
        X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)
        X_train = X_train_oversampled
        y_train = y_train_oversampled

        # Fit the model on the training data
        model.fit(X_train, y_train)

        # Predict the target variable for the test data
        y_pred = model.predict(X_test)

        # Calculate evaluation metrics for the fold
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        fscore = f1_score(y_test, y_pred)

        # Append the metrics to the fold lists
        fold_accuracies.append(accuracy)
        fold_precisions.append(precision)
        fold_recalls.append(recall)
        fold_fscores.append(fscore)

    # Calculate the mean metrics across all folds
    mean_accuracy = np.mean(fold_accuracies)
    mean_precision = np.mean(fold_precisions)
    mean_recall = np.mean(fold_recalls)
    mean_fscore = np.mean(fold_fscores)

    # Append the mean metrics to the overall lists
    model_names.append(model_name)
    accuracies.append(mean_accuracy)
    precisions.append(mean_precision)
    recalls.append(mean_recall)
    fscores.append(mean_fscore)

# Create a comparison table
results_df = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracies,
    'Precision': precisions,
    'Recall': recalls,
    'F1-score': fscores
})

# Print the comparison table
print("Comparison Table:")
display(results_df)
print()
print()

# Generate HTML content
html_content = """
<!DOCTYPE html>
<html>
<head>
  <title>Code Representation Evaluation Models</title>
  <style>
    body {
      font-family: Arial, sans-serif;
    }
    h1 {
      color: #333;
    }
    table {
      border-collapse: collapse;
      width: 100%;
    }
    th, td {
      padding: 8px;
      text-align: left;
      border-bottom: 1px solid #ddd;
    }
    th {
      background-color: #f2f2f2;
    }
    .plot {
      width: 600px;
      height: auto;
      margin-bottom: 20px;
    }
  </style>
</head>
<body>
  <h1>MLCQ God Class Metric Based Evaluation Models</h1>

  <h2>Comparison Table</h2>
  <table>
    <tr>
      <th>Model</th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1-score</th>
    </tr>
"""

# Add rows to the comparison table
for index, row in results_df.iterrows():
    html_content += f"""
    <tr>
      <td>{row['Model']}</td>
      <td>{row['Accuracy']:.4f}</td>
      <td>{row['Precision']:.4f}</td>
      <td>{row['Recall']:.4f}</td>
      <td>{row['F1-score']:.4f}</td>
    </tr>
"""

# Add bar plot
html_content += """
  </table>

  <h2>Bar Plot</h2>
  <div class="plot">
    <img src="bar_plot_god_metric.png" alt="Bar Plot">
  </div>
"""


# Add evaluation results section
html_content += """
  <h2>Evaluation Results</h2>
"""


# Add evaluation results for each model
for i in range(len(model_names)):
    html_content += f"""
    <h3>{model_names[i]}</h3>
    <ul>
      <li>Accuracy: {accuracies[i]:.4f}</li>
      <li>Precision: {precisions[i]:.4f}</li>
      <li>Recall: {recalls[i]:.4f}</li>
      <li>F1-score: {fscores[i]:.4f}</li>
    </ul>
"""

html_content += """
</body>
</html>
"""

# Save the HTML content to a file
with open("output_God_Class_Metric_Based.html", "w") as file:
    file.write(html_content)

# Save the bar plot as an image file
results_df.plot(kind='bar', x='Model', figsize=(10, 6))
plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Performance Comparison of Different Models')
plt.legend(loc='lower right')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig("bar_plot_god_metric.png")
plt.close()

print("Output saved successfully.")

"""
import joblib
# Save the trained model to a file
joblib.dump(models['SVM'], 'god_class_metric_based_SVM_model.pkl')

